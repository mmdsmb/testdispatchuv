{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/mustafa/Documents/MyWorkspaces/testdispatchuv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import base64\n",
    "from app.db.postgres import PostgresDataSource\n",
    "import logging\n",
    "from app.core.utils import clean_integer, decode_base64_credentials, get_google_drive_service, upload_to_drive, verifier_champs_manquants,SCOPES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = PostgresDataSource() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 20:37:13 - app.db.postgres - INFO - Connexion à la base de données établie avec succès\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 20:37:13,492 - app.db.postgres - INFO - Connexion à la base de données établie avec succès\n",
      "2025-05-11 20:37:13,609 - googleapiclient.discovery_cache - INFO - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapport généré et uploadé avec succès!\n",
      "Nombre total de lignes vérifiées: 74\n",
      "Nombre de lignes avec erreurs: 14\n"
     ]
    }
   ],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 02:17:13,099 - __main__ - INFO - Début du processus de vérification des hôtes\n",
      "2025-05-12 02:17:13,101 - __main__ - INFO - Exécution de la requête SQL pour récupérer les données des hôtes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-12 02:17:13 - app.db.postgres - INFO - Connexion à la base de données établie avec succès\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 02:17:13,321 - app.db.postgres - INFO - Connexion à la base de données établie avec succès\n",
      "2025-05-12 02:17:13,351 - __main__ - INFO - 74 hôtes récupérés depuis la base de données\n",
      "2025-05-12 02:17:13,354 - __main__ - INFO - Nettoyage des champs numériques\n",
      "2025-05-12 02:17:13,366 - __main__ - INFO - Vérification des données des hôtes\n",
      "2025-05-12 02:17:13,376 - __main__ - INFO - Vérification des champs obligatoires\n",
      "2025-05-12 02:17:13,402 - __main__ - INFO - Mise à jour des erreurs dans la table hotes\n",
      "2025-05-12 02:17:13,403 - __main__ - INFO - Début de la mise à jour des erreurs pour 74 hôtes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-12 02:17:13 - app.db.postgres - INFO - Transaction démarrée\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 02:17:13,415 - app.db.postgres - INFO - Transaction démarrée\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-12 02:17:14 - app.db.postgres - INFO - Transaction validée avec succès\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 02:17:14,088 - app.db.postgres - INFO - Transaction validée avec succès\n",
      "2025-05-12 02:17:14,089 - __main__ - INFO - Lot 74/74 traité avec succès\n",
      "2025-05-12 02:17:14,090 - __main__ - INFO - Mise à jour des erreurs terminée avec succès\n",
      "2025-05-12 02:17:14,094 - __main__ - INFO - 15 hôtes avec erreurs détectés\n",
      "2025-05-12 02:17:14,094 - __main__ - INFO - Génération du rapport d'erreurs: rapport_erreurs_20250512_021714.xlsx\n",
      "/var/folders/8v/kz5040m5025c51v8c045qhcr0000gn/T/ipykernel_4119/3334442658.py:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_erreurs['ID_erreur'] = df_erreurs['ID_erreur'].fillna('')\n",
      "/var/folders/8v/kz5040m5025c51v8c045qhcr0000gn/T/ipykernel_4119/3334442658.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_erreurs['Nombre-prs-AR_erreur'] = df_erreurs['Nombre-prs-AR_erreur'].fillna('')\n",
      "/var/folders/8v/kz5040m5025c51v8c045qhcr0000gn/T/ipykernel_4119/3334442658.py:159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_erreurs['Nombre-prs-Ret_erreur'] = df_erreurs['Nombre-prs-Ret_erreur'].fillna('')\n",
      "2025-05-12 02:17:14,173 - __main__ - INFO - Upload du rapport vers Google Drive (Dossier ID: 1cHSA8rGAk6sh-0qlfD3HVYUHnhi6wVTQ)\n",
      "2025-05-12 02:17:14,175 - googleapiclient.discovery_cache - INFO - file_cache is only supported with oauth2client<4.0.0\n",
      "2025-05-12 02:17:16,298 - __main__ - INFO - Rapport uploadé avec succès (File ID: 1DtUV4hXz7dLvSu99OaOKONQeMTRMRZHQ)\n",
      "2025-05-12 02:17:16,299 - __main__ - INFO - Processus terminé avec succès\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de lignes vérifiées: 74\n",
      "Nombre de lignes avec erreurs: 15\n"
     ]
    }
   ],
   "source": [
    "# V2 AVEC enregistrement dans la base de données\n",
    "\n",
    "# Configuration de la journalisation\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('hotes_verification.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "#from google.oauth2 import service_account\n",
    "\n",
    "# def get_google_drive_service():\n",
    "#     \"\"\"Version pour Service Account\"\"\"\n",
    "#     encoded_credentials = os.getenv('GOOGLE_CREDENTIALS_BASE64')\n",
    "#     if not encoded_credentials:\n",
    "#         raise ValueError(\"GOOGLE_CREDENTIALS_BASE64 manquant\")\n",
    "    \n",
    "#     credentials_dict = decode_base64_credentials(encoded_credentials)\n",
    "    \n",
    "#     if 'type' not in credentials_dict or credentials_dict['type'] != 'service_account':\n",
    "#         raise ValueError(\"Les identifiants doivent être pour un Service Account\")\n",
    "    \n",
    "#     creds = service_account.Credentials.from_service_account_info(\n",
    "#         credentials_dict,\n",
    "#         scopes=SCOPES\n",
    "#     )\n",
    "    \n",
    "#     return build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# def get_google_drive_service():\n",
    "#     \"\"\"Initialise et retourne le service Google Drive.\"\"\"\n",
    "#     creds = None\n",
    "    \n",
    "#     # Vérifie si le token existe déjà\n",
    "#     if os.path.exists('token.pickle'):\n",
    "#         with open('token.pickle', 'rb') as token:\n",
    "#             creds = pickle.load(token)\n",
    "    \n",
    "#     # Si les credentials n'existent pas ou sont invalides, demande une nouvelle authentification\n",
    "#     if not creds or not creds.valid:\n",
    "#         if creds and creds.expired and creds.refresh_token:\n",
    "#             creds.refresh(Request())\n",
    "#         else:\n",
    "#             # Récupérer les credentials encodés en base64 depuis .env\n",
    "#             encoded_credentials = os.getenv('GOOGLE_CREDENTIALS_BASE64')\n",
    "#             if not encoded_credentials:\n",
    "#                 raise ValueError(\"GOOGLE_CREDENTIALS_BASE64 non trouvé dans le fichier .env\")\n",
    "            \n",
    "#             # Décoder les credentials\n",
    "#             credentials_dict = decode_base64_credentials(encoded_credentials)\n",
    "            \n",
    "#             # Créer le flow avec les credentials\n",
    "#             flow = InstalledAppFlow.from_client_config(\n",
    "#                 credentials_dict, SCOPES)\n",
    "#             creds = flow.run_local_server(port=0)\n",
    "        \n",
    "#         # Sauvegarde les credentials pour la prochaine utilisation\n",
    "#         with open('token.pickle', 'wb') as token:\n",
    "#             pickle.dump(creds, token)\n",
    "    \n",
    "#     return build('drive', 'v3', credentials=creds)\n",
    "\n",
    "\n",
    "\n",
    "# def upload_to_drive(file_path, folder_id):\n",
    "#     \"\"\"Upload un fichier vers Google Drive dans le dossier spécifié.\"\"\"\n",
    "#     service = get_google_drive_service()\n",
    "    \n",
    "#     file_metadata = {\n",
    "#         'name': os.path.basename(file_path),\n",
    "#         'parents': [folder_id]\n",
    "#     }\n",
    "    \n",
    "#     media = MediaFileUpload(file_path, resumable=True)\n",
    "#     file = service.files().create(\n",
    "#         body=file_metadata,\n",
    "#         media_body=media,\n",
    "#         fields='id'\n",
    "#     ).execute()\n",
    "    \n",
    "#     return file.get('id')\n",
    "\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Fonction principale pour vérifier les données des hôtes et reporter les erreurs.\"\"\"\n",
    "    # Charger les variables d'environnement\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Récupérer l'ID du dossier depuis .env\n",
    "    FOLDER_ID = os.getenv('GOOGLE_DRIVE_FOLDER_ID')\n",
    "    if not FOLDER_ID:\n",
    "        logger.error(\"GOOGLE_DRIVE_FOLDER_ID non trouvé dans le fichier .env\")\n",
    "        raise ValueError(\"GOOGLE_DRIVE_FOLDER_ID non trouvé dans le fichier .env\")\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Début du processus de vérification des hôtes\")\n",
    "        \n",
    "        # 1. Récupération des données depuis la base\n",
    "        columns = [\"ID\", \"Prenom-Nom\", \"Telephone\", \"Nombre-prs-AR\", \"Provenance\", \n",
    "                  \"Arrivee-date\", \"Arrivee-vol\", \"Arrivee-heure\", \"Arrivee-Lieux\", \n",
    "                  \"Hebergeur\", \"RESTAURATION\", \"Telephone-hebergeur\", \"Adresse-hebergement\", \n",
    "                  \"Retour-date\", \"Nombre-prs-Ret\", \"Retour-vol\", \"Retour-heure\", \n",
    "                  \"Retour-Lieux\", \"Destination\", \"Chauffeur\", \"evenement_annee\", \"evenement_jour\"]\n",
    "        \n",
    "        query = \"\"\" \n",
    "        SELECT \"ID\", \"Prenom-Nom\", \"Telephone\", \"Nombre-prs-AR\", \"Provenance\",\n",
    "               \"Arrivee-date\", \"Arrivee-vol\", \"Arrivee-heure\", \"Arrivee-Lieux\",\n",
    "               \"Hebergeur\", \"RESTAURATION\", \"Telephone-hebergeur\", \"Adresse-hebergement\",\n",
    "               \"Retour-date\", \"Nombre-prs-Ret\", \"Retour-vol\", \"Retour-heure\",\n",
    "               \"Retour-Lieux\", \"Destination\", \"Chauffeur\", \"evenement_annee\", \"evenement_jour\"\n",
    "        FROM \"Hotes\" WHERE evenement_annee = 2024;\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(\"Exécution de la requête SQL pour récupérer les données des hôtes\")\n",
    "        hotes_data = await db.execute_query(query)\n",
    "        df = pd.DataFrame(hotes_data, columns=columns)\n",
    "        logger.info(f\"{len(df)} hôtes récupérés depuis la base de données\")\n",
    "\n",
    "        # Nettoyage des champs numériques\n",
    "        logger.info(\"Nettoyage des champs numériques\")\n",
    "        df['ID'], df['ID_erreur'] = zip(*df['ID'].apply(clean_integer))\n",
    "        df['Nombre-prs-AR'], df['Nombre-prs-AR_erreur'] = zip(*df['Nombre-prs-AR'].apply(clean_integer))\n",
    "        df['Nombre-prs-Ret'], df['Nombre-prs-Ret_erreur'] = zip(*df['Nombre-prs-Ret'].apply(clean_integer))\n",
    "\n",
    "        # 2. Vérification des données\n",
    "        logger.info(\"Vérification des données des hôtes\")\n",
    "        df_verifie = verifier_donnees_hotes(df)\n",
    "        \n",
    "        # 3. Mise à jour des erreurs dans la base de données\n",
    "        logger.info(\"Mise à jour des erreurs dans la table hotes\")\n",
    "        await update_hotes_errors(db, df_verifie)\n",
    "        \n",
    "        # 4. Filtrage des lignes avec erreurs\n",
    "        df_erreurs = df_verifie[\n",
    "            (df_verifie['erreur_aller'] != '') | \n",
    "            (df_verifie['erreur_retour'] != '') |\n",
    "            (df_verifie['ID_erreur'].notna()) |\n",
    "            (df_verifie['Nombre-prs-AR_erreur'].notna()) |\n",
    "            (df_verifie['Nombre-prs-Ret_erreur'].notna())\n",
    "        ]\n",
    "        logger.info(f\"{len(df_erreurs)} hôtes avec erreurs détectés\")\n",
    "\n",
    "        if not df_erreurs.empty:\n",
    "            # 5. Génération du rapport d'erreurs\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            nom_fichier = f'rapport_erreurs_{timestamp}.xlsx'\n",
    "            logger.info(f\"Génération du rapport d'erreurs: {nom_fichier}\")\n",
    "            \n",
    "            # Ajout des erreurs numériques dans le rapport\n",
    "            df_erreurs['ID_erreur'] = df_erreurs['ID_erreur'].fillna('')\n",
    "            df_erreurs['Nombre-prs-AR_erreur'] = df_erreurs['Nombre-prs-AR_erreur'].fillna('')\n",
    "            df_erreurs['Nombre-prs-Ret_erreur'] = df_erreurs['Nombre-prs-Ret_erreur'].fillna('')\n",
    "            \n",
    "            df_erreurs.to_excel(nom_fichier, index=False)\n",
    "            \n",
    "            # 6. Upload vers Google Drive\n",
    "            logger.info(f\"Upload du rapport vers Google Drive (Dossier ID: {FOLDER_ID})\")\n",
    "            file_id = upload_to_drive(nom_fichier, FOLDER_ID)\n",
    "            logger.info(f\"Rapport uploadé avec succès (File ID: {file_id})\")\n",
    "            \n",
    "            # 7. Nettoyage du fichier local\n",
    "            #os.remove(nom_fichier)\n",
    "            #logger.info(\"Fichier local nettoyé\")\n",
    "        else:\n",
    "            logger.info(\"Aucune erreur détectée - aucun rapport généré\")\n",
    "\n",
    "        logger.info(\"Processus terminé avec succès\")\n",
    "        print(f\"Nombre total de lignes vérifiées: {len(df)}\")\n",
    "        print(f\"Nombre de lignes avec erreurs: {len(df_erreurs)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Une erreur est survenue: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "async def update_hotes_errors(db, df_verifie, batch_size=100):\n",
    "    \"\"\"Met à jour les champs d'erreur dans la table hotes par lots\"\"\"\n",
    "    logger.info(f\"Début de la mise à jour des erreurs pour {len(df_verifie)} hôtes\")\n",
    "    \n",
    "    total_rows = len(df_verifie)\n",
    "    processed = 0\n",
    "    \n",
    "    while processed < total_rows:\n",
    "        batch = df_verifie.iloc[processed:processed+batch_size]\n",
    "        transactions = []\n",
    "        \n",
    "        for _, row in batch.iterrows():\n",
    "            # Combiner les erreurs standards et les erreurs numériques\n",
    "            erreur_aller = row['erreur_aller']\n",
    "            if row['ID_erreur']:\n",
    "                erreur_aller += f\", ID: {row['ID_erreur']}\" if erreur_aller else f\"ID: {row['ID_erreur']}\"\n",
    "            if row['Nombre-prs-AR_erreur']:\n",
    "                erreur_aller += f\", Nombre-prs-AR: {row['Nombre-prs-AR_erreur']}\" if erreur_aller else f\"Nombre-prs-AR: {row['Nombre-prs-AR_erreur']}\"\n",
    "            \n",
    "            erreur_retour = row['erreur_retour']\n",
    "            if row['Nombre-prs-Ret_erreur']:\n",
    "                erreur_retour += f\", Nombre-prs-Ret: {row['Nombre-prs-Ret_erreur']}\" if erreur_retour else f\"Nombre-prs-Ret: {row['Nombre-prs-Ret_erreur']}\"\n",
    "            \n",
    "            update_query = \"\"\"\n",
    "                UPDATE \"Hotes\" SET\n",
    "                    erreur_aller = NULLIF(%s, ''),\n",
    "                    erreur_retour = NULLIF(%s, '')\n",
    "                WHERE \"ID\" = %s\n",
    "            \"\"\"\n",
    "            params = [\n",
    "                erreur_aller,\n",
    "                erreur_retour,\n",
    "                row['ID']\n",
    "            ]\n",
    "            transactions.append((update_query, params))\n",
    "        \n",
    "        try:\n",
    "            await db.execute_transaction(transactions)\n",
    "            processed += len(batch)\n",
    "            logger.info(f\"Lot {processed}/{total_rows} traité avec succès\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du lot {processed}-{processed+batch_size}\", exc_info=True)\n",
    "            raise\n",
    "    \n",
    "    logger.info(\"Mise à jour des erreurs terminée avec succès\")\n",
    "\n",
    "def verifier_donnees_hotes(df):\n",
    "    \"\"\"Vérifie les données des hôtes et ajoute les colonnes d'erreurs.\"\"\"\n",
    "    logger.info(\"Vérification des champs obligatoires\")\n",
    "    \n",
    "    df_verifie = df.copy()\n",
    "    \n",
    "    # Champs obligatoires pour l'aller\n",
    "    champs_aller = ['ID', 'Prenom-Nom', 'Nombre-prs-AR', 'Arrivee-Lieux', \n",
    "                   'Adresse-hebergement', 'Arrivee-date', 'Arrivee-heure']\n",
    "    \n",
    "    # Champs obligatoires pour le retour\n",
    "    champs_retour = ['ID', 'Prenom-Nom', 'Nombre-prs-Ret', 'Retour-Lieux', \n",
    "                    'Retour-date', 'Retour-heure']\n",
    "    \n",
    "    # Vérification des champs\n",
    "    df_verifie['erreur_aller'] = df_verifie.apply(\n",
    "        lambda row: verifier_champs_manquants(row, champs_aller), axis=1)\n",
    "    df_verifie['erreur_retour'] = df_verifie.apply(\n",
    "        lambda row: verifier_champs_manquants(row, champs_retour), axis=1)\n",
    "    \n",
    "    return df_verifie\n",
    "\n",
    "\n",
    "\n",
    "# Exécution dans Jupyter Notebook\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 SANS enregistrement dans la base de données\n",
    "\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "# load_dotenv()\n",
    "\n",
    "# # Configuration des scopes pour Google Drive API\n",
    "# SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "\n",
    "# def decode_base64_credentials(encoded_credentials):\n",
    "#     \"\"\"Décode les credentials encodés en base64.\"\"\"\n",
    "#     try:\n",
    "#         # Décoder la chaîne base64\n",
    "#         decoded_bytes = base64.b64decode(encoded_credentials)\n",
    "#         # Convertir en chaîne de caractères\n",
    "#         decoded_str = decoded_bytes.decode('utf-8')\n",
    "#         # Parser le JSON\n",
    "#         return json.loads(decoded_str)\n",
    "#     except Exception as e:\n",
    "#         raise ValueError(f\"Erreur lors du décodage des credentials: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from google.oauth2 import service_account\n",
    "\n",
    "# def get_google_drive_service():\n",
    "#     \"\"\"Version pour Service Account\"\"\"\n",
    "#     encoded_credentials = os.getenv('GOOGLE_CREDENTIALS_BASE64')\n",
    "#     if not encoded_credentials:\n",
    "#         raise ValueError(\"GOOGLE_CREDENTIALS_BASE64 manquant\")\n",
    "    \n",
    "#     credentials_dict = decode_base64_credentials(encoded_credentials)\n",
    "    \n",
    "#     if 'type' not in credentials_dict or credentials_dict['type'] != 'service_account':\n",
    "#         raise ValueError(\"Les identifiants doivent être pour un Service Account\")\n",
    "    \n",
    "#     creds = service_account.Credentials.from_service_account_info(\n",
    "#         credentials_dict,\n",
    "#         scopes=SCOPES\n",
    "#     )\n",
    "    \n",
    "#     return build('drive', 'v3', credentials=creds)\n",
    "# # def get_google_drive_service():\n",
    "# #     \"\"\"Initialise et retourne le service Google Drive.\"\"\"\n",
    "# #     creds = None\n",
    "    \n",
    "# #     # Vérifie si le token existe déjà\n",
    "# #     if os.path.exists('token.pickle'):\n",
    "# #         with open('token.pickle', 'rb') as token:\n",
    "# #             creds = pickle.load(token)\n",
    "    \n",
    "# #     # Si les credentials n'existent pas ou sont invalides, demande une nouvelle authentification\n",
    "# #     if not creds or not creds.valid:\n",
    "# #         if creds and creds.expired and creds.refresh_token:\n",
    "# #             creds.refresh(Request())\n",
    "# #         else:\n",
    "# #             # Récupérer les credentials encodés en base64 depuis .env\n",
    "# #             encoded_credentials = os.getenv('GOOGLE_CREDENTIALS_BASE64')\n",
    "# #             if not encoded_credentials:\n",
    "# #                 raise ValueError(\"GOOGLE_CREDENTIALS_BASE64 non trouvé dans le fichier .env\")\n",
    "            \n",
    "# #             # Décoder les credentials\n",
    "# #             credentials_dict = decode_base64_credentials(encoded_credentials)\n",
    "            \n",
    "# #             # Créer le flow avec les credentials\n",
    "# #             flow = InstalledAppFlow.from_client_config(\n",
    "# #                 credentials_dict, SCOPES)\n",
    "# #             creds = flow.run_local_server(port=0)\n",
    "        \n",
    "# #         # Sauvegarde les credentials pour la prochaine utilisation\n",
    "# #         with open('token.pickle', 'wb') as token:\n",
    "# #             pickle.dump(creds, token)\n",
    "    \n",
    "# #     return build('drive', 'v3', credentials=creds)\n",
    "\n",
    "\n",
    "\n",
    "# def upload_to_drive(file_path, folder_id):\n",
    "#     \"\"\"Upload un fichier vers Google Drive dans le dossier spécifié.\"\"\"\n",
    "#     service = get_google_drive_service()\n",
    "    \n",
    "#     file_metadata = {\n",
    "#         'name': os.path.basename(file_path),\n",
    "#         'parents': [folder_id]\n",
    "#     }\n",
    "    \n",
    "#     media = MediaFileUpload(file_path, resumable=True)\n",
    "#     file = service.files().create(\n",
    "#         body=file_metadata,\n",
    "#         media_body=media,\n",
    "#         fields='id'\n",
    "#     ).execute()\n",
    "    \n",
    "#     return file.get('id')\n",
    "\n",
    "# def verifier_champs_manquants(row, champs_a_verifier):\n",
    "#     \"\"\"Vérifie les champs manquants pour une ligne donnée.\"\"\"\n",
    "#     champs_manquants = []\n",
    "#     for champ in champs_a_verifier:\n",
    "#         if pd.isna(row[champ]) or str(row[champ]).strip() == '':\n",
    "#             champs_manquants.append(champ)\n",
    "#     return ', '.join(champs_manquants) if champs_manquants else ''\n",
    "\n",
    "# def verifier_donnees_hotes(df):\n",
    "#     \"\"\"Vérifie les données des hôtes et ajoute les colonnes d'erreurs.\"\"\"\n",
    "#     # Copie du DataFrame pour éviter de modifier l'original\n",
    "#     df_verifie = df.copy()\n",
    "    \n",
    "#     # Définition des champs à vérifier pour l'aller et le retour\n",
    "#     champs_aller = ['ID', 'Prenom-Nom', 'Nombre-prs-AR', 'Arrivee-Lieux', \n",
    "#                     'Adresse-hebergement', 'Arrivee-date', 'Arrivee-heure']\n",
    "#     champs_retour = ['ID', 'Prenom-Nom', 'Nombre-prs-Ret', 'Retour-Lieux', \n",
    "#                      'Retour-date', 'Retour-heure']\n",
    "    \n",
    "#     # Ajout des colonnes d'erreurs\n",
    "#     df_verifie['erreur_aller'] = df_verifie.apply(\n",
    "#         lambda row: verifier_champs_manquants(row, champs_aller), axis=1)\n",
    "#     df_verifie['erreur_retour'] = df_verifie.apply(\n",
    "#         lambda row: verifier_champs_manquants(row, champs_retour), axis=1)\n",
    "    \n",
    "#     return df_verifie\n",
    "\n",
    "# async def main():\n",
    "#     # Récupérer l'ID du dossier depuis .env\n",
    "#     FOLDER_ID = os.getenv('GOOGLE_DRIVE_FOLDER_ID')\n",
    "#     if not FOLDER_ID:\n",
    "#         raise ValueError(\"GOOGLE_DRIVE_FOLDER_ID non trouvé dans le fichier .env\")\n",
    "    \n",
    "#     try:\n",
    "#         # Lecture du fichier Excel des hôtes\n",
    "#         #df = pd.read_excel('hotes.xlsx')\n",
    "#         # Récupérer les données de la table Hotes avec execute_query\n",
    "#         columns = [\"ID\", \"Prenom-Nom\", \"Telephone\", \"Nombre-prs-AR\", \"Provenance\", \"Arrivee-date\", \"Arrivee-vol\", \"Arrivee-heure\", \"Arrivee-Lieux\", \"Hebergeur\", \"RESTAURATION\", \"Telephone-hebergeur\", \"Adresse-hebergement\", \"Retour-date\", \"Nombre-prs-Ret\", \"Retour-vol\", \"Retour-heure\", \"Retour-Lieux\", \"Destination\", \"Chauffeur\", \"evenement_annee\", \"evenement_jour\"]\n",
    "#         query = \"\"\" \n",
    "#         SELECT  \"ID\" ,\n",
    "#             \"Prenom-Nom\" ,\n",
    "#             \"Telephone\" ,\n",
    "#             \"Nombre-prs-AR\" ,\n",
    "#             \"Provenance\" ,\n",
    "#             \"Arrivee-date\" ,\n",
    "#             \"Arrivee-vol\" ,\n",
    "#             \"Arrivee-heure\" ,\n",
    "#             \"Arrivee-Lieux\" ,\n",
    "#             \"Hebergeur\" ,\n",
    "#             \"RESTAURATION\" ,\n",
    "#             \"Telephone-hebergeur\" ,\n",
    "#             \"Adresse-hebergement\" ,\n",
    "#             \"Retour-date\" ,\n",
    "#             \"Nombre-prs-Ret\" ,\n",
    "#             \"Retour-vol\" ,\n",
    "#             \"Retour-heure\" ,\n",
    "#             \"Retour-Lieux\" ,\n",
    "#             \"Destination\" ,\n",
    "#             \"Chauffeur\" ,\n",
    "#             \"evenement_annee\" ,\n",
    "#             \"evenement_jour\"\n",
    "#             FROM \"Hotes\" where evenement_annee = 2024\n",
    "#             ;\n",
    "#             \"\"\"\n",
    "        \n",
    "#         hotes_data = await db.execute_query(query)\n",
    "\n",
    "#         # Convertir les données en DataFrame\n",
    "#         df = pd.DataFrame(hotes_data, columns=columns)\n",
    "#         # Vérification des données\n",
    "#         df_verifie = verifier_donnees_hotes(df)\n",
    "        \n",
    "#         # Filtrage des lignes avec erreurs\n",
    "#         df_erreurs = df_verifie[\n",
    "#             (df_verifie['erreur_aller'] != '') | \n",
    "#             (df_verifie['erreur_retour'] != '')\n",
    "#         ]\n",
    "        \n",
    "#         # Génération du nom de fichier avec timestamp\n",
    "#         timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#         nom_fichier = f'rapport_erreurs_{timestamp}.xlsx'\n",
    "        \n",
    "#         # Sauvegarde du fichier Excel\n",
    "#         df_erreurs.to_excel(nom_fichier, index=False)\n",
    "        \n",
    "#         # Upload vers Google Drive\n",
    "#         file_id = upload_to_drive(nom_fichier, FOLDER_ID)\n",
    "        \n",
    "#         print(f\"Rapport généré et uploadé avec succès!\")\n",
    "#         print(f\"Nombre total de lignes vérifiées: {len(df)}\")\n",
    "#         print(f\"Nombre de lignes avec erreurs: {len(df_erreurs)}\")\n",
    "        \n",
    "#         # Nettoyage du fichier local\n",
    "#         os.remove(nom_fichier)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Une erreur est survenue: {str(e)}\")\n",
    "\n",
    "\n",
    "# # Exécution dans Jupyter Notebook\n",
    "# await main()  # Remplacez asyncio.run(main()) par cette ligne"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
